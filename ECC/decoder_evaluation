import numpy as np
import keras
import tensorflow
import gym

from Function_Library import *
from Environments import *

import rl as rl
from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy, GreedyQPolicy
from rl.memory import SequentialMemory
from rl.callbacks import FileLogger

import json
import copy
import sys
import os
import shutil
import datetime

fixed_configs = {
    "d": 5, # Lattice Width
    "use_Y": False, # true => agent performs Pauli Y, false => only X and Z
    "train_freq": 1, # number of interaction steps between W updates
    "batch_size": 32,
    "print_freq": 250,
    "rolling_average_length": 500,
    "stopping_patience": 500,
    "error_model": "X", # Noise model: X flips or DP (depolarising noise)
    "c_layers": [[64, 3, 2], [32, 2, 1], [32, 2, 1]], # conv. layers of deeqQ net
    "ff_layers": [[512, 0.2]], # layers of ff net
    "max_timesteps": 100000,
    "volume_depth": 5, # number of syndrome measures for each extraction
    "testing_length": 101,
    "buffer_size": 50000,
    "dueling": True,
    "masked_greedy": False,
    "static_decoder": True # Should be True when within fault tolerant setting
}

variable_configs = {
    "p_phys": 0.001, # phys error prob
    "p_meas": 0.001, # meas error prob
    "success_threshold": 10000, # qubit lifetime at which success
    "learning_starts": 1000,
    "learning_rate": 0.00001,
    "exploration_fraction": 100000,
    "max_eps": 1.0,
    "target_network_update_freq": 5000, # clone target network off deepQ agent (generates target Qfunc)
    "gamma": 0.99, # discount rate for calc Qvals
    "final_eps": 0.02
}
model_weights_path = os.path.join(os.getcwd(),"../logging_dir/dqn_weights.h5f")

static_decoder_path = os.path.join(os.getcwd(),"referee_decoders/nn_d5_X_p5")
static_decoder = load_model(static_decoder_path)

all_configs = {}

for key in fixed_configs.keys():
    all_configs[key] = fixed_configs[key]

for key in variable_configs.keys():
    all_configs[key] = variable_configs[key]

env = Surface_Code_Environment_Multi_Decoding_Cycles(d=all_configs["d"], 
    p_phys=all_configs["p_phys"], 
    p_meas=all_configs["p_meas"],  
    error_model=all_configs["error_model"], 
    use_Y=all_configs["use_Y"], 
    volume_depth=all_configs["volume_depth"],
    static_decoder=static_decoder)

model = build_convolutional_nn(all_configs["c_layers"],all_configs["ff_layers"], 
                               env.observation_space.shape, env.num_actions)
memory = SequentialMemory(limit=all_configs["buffer_size"], window_length=1)
policy = GreedyQPolicy(masked_greedy=True)
test_policy = GreedyQPolicy(masked_greedy=True)

dqn = DQNAgent(model=model, 
               nb_actions=env.num_actions, 
               memory=memory, 
               nb_steps_warmup=all_configs["learning_starts"], 
               target_model_update=all_configs["target_network_update_freq"], 
               policy=policy,
               test_policy=test_policy,
               gamma = all_configs["gamma"],
               enable_dueling_network=all_configs["dueling"])  
dqn.compile(Adam(lr=all_configs["learning_rate"]))
dqn.model.load_weights(model_weights_path)

d=all_configs["d"]
p_phys=all_configs["p_phys"]
p_meas=p_phys
error_model = "X"
qubits = generateSurfaceCodeLattice(d)

hidden_state = np.zeros((d, d), int)

faulty_syndromes = []
for j in range(d):
    error = generate_error(d, p_phys, error_model)
    hidden_state = obtain_new_error_configuration(hidden_state, error)
    current_true_syndrome = generate_surface_code_syndrome_NoFT_efficient(hidden_state, qubits)
    current_faulty_syndrome = generate_faulty_syndrome(current_true_syndrome, p_meas)
    faulty_syndromes.append(current_faulty_syndrome)

input_state = np.zeros((d+1, 2*d + 1, 2*d + 1),int)

# embed and place the faulty syndrome slices in the correct place
for j in range(d):
            input_state[j, :, :] = env.padding_syndrome(faulty_syndromes[j])

corrections = []

still_decoding = True
while still_decoding:
    
    # Fetch the suggested correction
    action = dqn.forward(input_state)
    
    if action not in corrections and action != env.identity_index:
        # If the action has not yet been done, or is not the identity
        
        # append the suggested correction to the list of corrections
        corrections.append(action)
        
        # Update the input state to the agent to indicate the correction it would have made
        input_state[d, :, :] = env.padding_actions(corrections)
        
    else:
        # decoding should stop
        still_decoding = False

print(corrections)